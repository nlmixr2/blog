---
title: nlmixr2 2.0.8 log-likelihood
author: 'Matt Fidler and the nlmixr2 Development Team'
date: '2022-10-23'
slug: []
bibliography: [refs.bib]
link-citations: true
csl: vancouver.csl
categories: [nlmixr2]
tags: [new-version]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rxode2)
library(tidyverse)
library(nlmixr2)
```

I am pretty excited abut the new `nlmixr2` release (2.0.8). When I joined the the nlmixr2 team, I wanted to do a fancy heavy-tailed, skewed model in an open source tool so I could figure out how to do even more with it.

With this release, it is possible to do a heavy-tailed (t-distribution `dt()`), skewed (`coxBox(lambda)`) distribution: my old wish is now possible with `focei`!

A few other things that people may be interested in are:

-   Likelihood based each observation (and how to get it)

-   Standard errors for etas (and how to get them)

-   More robust dose-based between-subject variability (lag time, bioavailability, modeled rate/duration)

These will all be discussed in other posts (since this one is long already).

# Testing Generalized Likelihood

With any new method, it's important that the results make sense.

With that in mind, I thought I could use the models Rik used to compare the SAEM and FOCEi algorithms in Monolix and NONMEM, respectively [@Schoemaker2019] to check the generalized likelihood.

In these tests, we can modify the error model to use generalized likelihood by adding a `+ dnorm()` to the end of the residual specification. This allows us to run a generalized likelihood and compare it to the compare the current `focei` results.

As discussed in Rik's paper, every model has estimated a central volume. So simply looking at `Vc` and how it behaves is a good surrogate of how well this method is doing.

First, we will compare the population estimated `Vc` values between the methods and against the true value of `70`:

```{r VcPlot, echo=FALSE}
ret <-qs::qread("run.qs")
ret$est <- gsub("nonmem743", "NONMEM 7.4.3",
                gsub("foceiLL", "focei log-likelihood", ret$est))
ret$run <- factor(ret$run)
ret$by <- ret$est
ret$run2 <- as.integer(ret$run)
.lvl <- levels(ret$run)
.brk <- seq_along(.lvl);
ggplot(ret, aes(run2, Vc, color=by))+
  geom_point(size=3, alpha=0.5) +
  geom_line(alpha=0.5, size=1.3) +
  geom_hline(yintercept=70, size=1.3) +
  theme_bw() +
  theme(axis.text.x = element_text(face="bold", angle=45, hjust=1),
        axis.title.y=element_text(face="bold", size=14),
        axis.title.x=element_blank(),
        legend.position="top") +
  labs(color="") +
  scale_x_continuous(breaks=.brk,labels=.lvl, minor_breaks=NULL) +
  ylab("Vc (L)")
```

In my estimation, the `Vc` values are similar between the two methods and both vary (similarly) around the true value of `70`. Incidentally both Monolix and NONMEM give similar findings here [@Schoemaker2019].

The next parameter to check is the between subject variability on `Vc` where the true value is 30% (CV):

```{r bsvVc, echo=FALSE}
ggplot(ret, aes(run2, BSV.Vc, color=by))+
  geom_point(size=3, alpha=0.5) +
  geom_line(alpha=0.5, size=1.3) +
  geom_hline(yintercept=30, size=1.3) +
  theme_bw() +
  theme(axis.text.x = element_text(face="bold", angle=45, hjust=1),
        axis.title.y=element_text(face="bold", size=14),
        axis.title.x=element_blank(),
        legend.position="top") +
  labs(color="") +
  scale_x_continuous(breaks=.brk,labels=.lvl, minor_breaks=NULL) +
  ylab("Between Subject on Vc (CV%)")
```

The estimates here also look fairly consistent (and reasonable).

Now let's examine the standard errors of the estimates:

```{r seV, echo=FALSE}
library(nlmixr2)
library(dplyr)
df2 <- ret %>% mutate(SE.VM = ifelse(covMethod=="r,s", SE.VM, NA),
                          SE.KM = ifelse(covMethod=="r,s", SE.KM, NA),
                          SE.Q  = ifelse(covMethod=="r,s", SE.Q, NA),
                          SE.Vp = ifelse(covMethod=="r,s", SE.Vp, NA),
                          SE.KA = ifelse(covMethod=="r,s", SE.KA, NA),
                          SE.Cl = ifelse(covMethod=="r,s", SE.Cl, NA),
                          SE.Vc = ifelse(covMethod=="r,s", SE.Vc, NA))

ggplot(df2, aes(run2, SE.Vc, color=by, group=by)) +
  geom_point(size=3, alpha=0.5) +
  geom_line(alpha=0.5, size=1.3) +
  theme_bw() +
  theme(axis.text.x = element_text(face="bold", angle=45, hjust=1),
        axis.title.y=element_text(face="bold", size=14),
        axis.title.x=element_blank(),
        legend.position="none") +
  scale_x_continuous(breaks=.brk, labels=.lvl, minor_breaks=NULL) +
  ylab(paste0("SE log Vc (L) (r,s)"))

```

Here you can see:

-   The resulting estimates are similar, but often higher for the log-likelihood estimation

-   Log-likelihood estimation is less likely to have a successful covariance step (as measured by a `r,s` matrix)

If you allow all the covariance types returned, you can see a similar pattern:

```{r seVcAll, echo=FALSE}
ggplot(ret, aes(run2, SE.Vc, color=by, group=by)) +
  geom_point(size=3, alpha=0.5) +
  geom_line(alpha=0.5, size=1.3) +
  theme_bw() +
  theme(axis.text.x = element_text(face="bold", angle=45, hjust=1),
        axis.title.y=element_text(face="bold", size=14),
        axis.title.x=element_blank(),
        legend.title=element_blank(),
        legend.position="top") +
  scale_x_continuous(breaks=.brk, labels=.lvl, minor_breaks=NULL) +
  ylab(paste0("SE log Vc (L) (all)"))

```

The next question is: how much (longer) does it take to run `nlmixr2` with log-likelihood than with a standard normal model?

```{r runTime, echo=FALSE}
d1 <- ret %>%
  filter(by=="focei") %>%
  select(run2, time) %>%
  rename(timeFocei=time)

d2 <- ret %>%
  filter(by != "focei") %>%
  select(run2, time) %>%
  rename(timeFoceiLl=time)

d <- merge(d1, d2) %>%
  mutate(ratio=timeFoceiLl/timeFocei)

ggplot(d, aes(run2, ratio)) +
  geom_point(size=3, alpha=0.5) +
  geom_line(alpha=0.5, size=1.3) +
  theme_bw() +
  theme(axis.text.x = element_text(face="bold", angle=45, hjust=1),
        axis.title.y=element_text(face="bold", size=14),
        axis.title.x=element_blank(),
        legend.position="top",
        legend.title=element_blank()) +
  scale_x_continuous(breaks=.brk, labels=.lvl, minor_breaks=NULL) +
  ylab(paste0("Ratio of time(log-likelihod)/time(focei)"))
```

This shows it takes anywhere from 0.8 to 7.2 fold longer to use the log-likelihood method than the standard `focei` method with these datasets.

This is because the eta Hessian must be numerically estimated for each individual subject to in order to calculate the overall objective function. The time difference is likely a function of:

1.  number of between-subject varaibilities estimated

2.  number of subjects in the dataset

**NOTE:** Because the log-likelihood `focei` calculation is different from the NONMEM approximation of the individual Hessian (as described in detail by Almquist [@Almquist2015]), **you should not perform statistical comparisons between a log-likelihood model and a standard `focei` model; the calculations are different, and it is not clear that the differences are due to likelihood alone.**

That being said, the generalized log-likelihood method ***does*** approximate the same likelihood as the NOMMEM-style `focei`. Hence, plotting the objective functions between the two methods give very similar values for each problem:

```{r objf, echo=FALSE}
ggplot(ret, aes(run2, objf, color=by, group=by)) +
  geom_point(size=3, alpha=0.5) +
  geom_line(alpha=0.5, size=1.3) +
  theme_bw() +
  theme(axis.text.x = element_text(face="bold", angle=45, hjust=1),
        axis.title.y=element_text(face="bold", size=14),
        axis.title.x=element_blank(),
        legend.position="top",
        legend.title=element_blank()) +
  xgxr::xgx_scale_y_log10() +
  scale_x_continuous(breaks=.brk, labels=.lvl, minor_breaks=NULL) +
  ylab(paste0("Objective Function"))
```

### Overall Conclusions about `nlmixr2` log-likelihood `focei`

-   The log-likelihood estimation procedure performs well for estimating population effects and between-subject variabilities.

-   The log-likelihood estimation procedure provides standard error estimates that are in the right ballpark, but tend to be a bit larger than the true values.

-   The procedure takes more time than `focei` (0.8 to 7-fold more)

-   You should not compare objective functions between models that were estimated by general `focei` and log-likelihood `focei`.

# Generalized likelihood methods in `nlmixr2`

### Normal and t-related distributions

For the normal and t-related distributions I wanted to keep the ability to use skewed distributions additive and proportional in the t-space (etc), so these distributions are specified differently in comparison to the other supported distributions within `nlmixr2`:

+-------------------------+----------------+----------------------------+
| Distribution            | How to Add     | Example                    |
+=========================+================+============================+
| Normal (log-likelihood) | `+dnorm()`     | `cp~add(add.sd)+dnorm()`   |
+-------------------------+----------------+----------------------------+
| T-distribuion           | `+dt(df)`      | `cp~add(add.sd)+dt(df)`    |
+-------------------------+----------------+----------------------------+
| Cauchy\                 | `+dcauchy()`   | `cp~add(add.sd)+dcauchy()` |
| (t with df=1)           |                |                            |
+-------------------------+----------------+----------------------------+

Note that with the normal and t-related distributions `nlmixr2` will calculate `cwres` and `npde` under the normal assumption to help assess the goodness of the fit of the model.

### More distributions

These (mostly) match all match the R distributions in the function. The left-hand side of the equation (`err`) represents the compartment name for the `DV` in the dataset (if needed). It still needs the compartment to be defined even if it is a single endpoint model.

The only function that does not exactly reflect the corresponding R documentation is the `dnbinomMu` probability function. It takes the `size` and `mu` as documented below and in the R documentation.

+---------------------------------+-----------------------------+
| Distribution                    | How to Add                  |
+=================================+=============================+
| Poisson                         | `err ~ dpois(lamba)`        |
+---------------------------------+-----------------------------+
| Binomial                        | `err ~ dbinom(n, p)`        |
+---------------------------------+-----------------------------+
| Beta                            | `err ~ dbeta(alpha, beta)`  |
+---------------------------------+-----------------------------+
| Chi-squared                     | `err ~ chisq(nu)`           |
+---------------------------------+-----------------------------+
| Exponential                     | `err ~ dexp(r)`             |
+---------------------------------+-----------------------------+
| Uniform                         | `err ~ dunif(a, b)`         |
+---------------------------------+-----------------------------+
| Weibull                         | `err ~ dweibull(a, b)`      |
+---------------------------------+-----------------------------+
| Gamma                           | `err ~ dgamma(a, b)`        |
+---------------------------------+-----------------------------+
| Geometric                       | `err ~ dgeom(a)`            |
+---------------------------------+-----------------------------+
| Negative Binomial               | `err ~ dnbinom(n, p)`       |
+---------------------------------+-----------------------------+
| Negative Binomial (mu version)  | `err ~ dnbinomMu(size, mu)` |
+---------------------------------+-----------------------------+

### Ordinal

Finally, ordinal likelihoods/simulations can be specified in 2 ways. The first is:

``` r
err ~ c(p0, p1, p2)
```

Here `err` represents the compartment and `p0` is the probability of being in a specific category:

| Category | Probability |
|----------|-------------|
| 1        | p0          |
| 2        | p1          |
| 3        | p2          |
| 4        | 1-p0-p1-p2  |

It is up to the model to ensure that the sum of the `p` values are less than `1`. Additionally you can write an arbitrary number of categories in the ordinal model described above.

It seems a little off that `p0` is the probability for category `1` and sometimes scores are in non-whole numbers. This can be modeled as follows:

``` r
err ~ c(p0=0, p1=1, p2=2, 3)
```

Here the numeric categories are specified explicitly, and the probabilities remain the same:

| Category | Probability |
|----------|-------------|
| 0        | p0          |
| 1        | p1          |
| 2        | p2          |
| 3        | 1-p0-p1-p2  |

### Generalized likelihood

The generalized log-likelihood can be defined as follows:

``` r
ll(err) ~ log likelihood expression
```

Here `err` represents the compartment or `dvid` that specifies the log-likelihood. You still need to specify it even if it is a single endpoint model (like the distributions above).

### References
