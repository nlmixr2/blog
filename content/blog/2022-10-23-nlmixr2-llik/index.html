---
title: nlmixr2 2.0.8 log-likelihood
author: 'Matt Fidler and the nlmixr2 Development Team'
date: '2022-10-24'
slug: []
bibliography: [refs.bib]
link-citations: true
csl: vancouver.csl
categories: [nlmixr2]
tags: [new-version]
---


<p>I am pretty excited abut the new nlmixr2 release (2.0.8). When I joined the the nlmixr2 team, I wanted to do a fancy heavy tailed, skewed model in an open source tool so I could figure out how to do even more with it.</p>
<p>With this release, it is possible to do a heavy tailed (t-distribution <code>dt()</code>) skewed (<code>coxBox(lambda)</code>) distribution: my old wish is now possible with <code>focei</code>!</p>
<p>A few other things that people may be interested in are:</p>
<ul>
<li><p>Likelihood based each observation (and how to get it)</p></li>
<li><p>Standard Errors for etas (and how to get it)</p></li>
<li><p>More robust dose-based between subject variability (lag time, bioavailability, modeled rate/duration)</p></li>
</ul>
<p>These will all be discussed in other posts (since this one is long already).</p>
<div id="testing-generalized-likelihood" class="section level1">
<h1>Testing Generalized Likelihood</h1>
<p>With any new method I want to the results makes sense.</p>
<p>With that in mind, I thought I could use the models Rik used to compare the SAEM and FOCEi algorithms to Monolix and NONMEM, respectively <span class="citation">(<a href="#ref-Schoemaker2019" role="doc-biblioref">1</a>)</span> to check the generalized likelihood.</p>
<p>In these tests, we can modify the error model to use generalized likelihood by adding a <code>+ dnorm()</code> to the end of the residual specification. This allows us to run a generalized likelihood and compare it to the compare the current <code>focei</code> .</p>
<p>As discussed in Rikâ€™s paper, every model has estimated a central volume. So simply looking at <code>Vc</code> and how it behaves is a good surrogate of how well this method is doing.</p>
<p>First, we will compare the population estimated <code>Vc</code> values between the methods and against the true value of <code>70</code>:</p>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/VcPlot-1.png" width="672" /></p>
<p>In my estimation, the <code>Vc</code> values are similar between the two methods and both vary (similarly) around the true value of <code>70</code>. Incidentally both Monolix and NONMEM give similar findings here <span class="citation">(<a href="#ref-Schoemaker2019" role="doc-biblioref">1</a>)</span>.</p>
<p>The next parameter to check is the between subject variability on <code>Vc</code> where the true value is 30%CV:</p>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/bsvVc-1.png" width="672" /></p>
<p>The estimates here also look fairly consistent (and reasonable).</p>
<p>Now lets examine the standard errors of the estimates:</p>
<pre><code>## Warning: Removed 6 rows containing missing values (geom_point).</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/seV-1.png" width="672" /></p>
<p>Here you can see:</p>
<ul>
<li><p>The results are similar, but often higher for the log-likelihood estimation</p></li>
<li><p>The log-likelihood estimation is less likely to have a successful covariance (as measured by a <code>r,s</code> matrix)</p></li>
</ul>
<p>If you allow all the covariance types returned, you can see a similar pattern:</p>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/seVcAll-1.png" width="672" /></p>
<p>The next question is how much (longer) does it take to run <code>nlmixr2</code> with log-likelihood than with a standard normal model</p>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/runTime-1.png" width="672" /></p>
<p>This shows it takes anywhere from 0.8 to 7.2 fold longer to use the log-likelihood method than the standard focei method with these datasets.</p>
<p>This is because each individual has to estimate the eta Hessian numerically for each subject to calculate the overall objective function. The time difference is likely a function of:</p>
<ol style="list-style-type: decimal">
<li><p>number of between subject varaibilities estimated</p></li>
<li><p>number of subjects in the data-set</p></li>
</ol>
<p><strong>NOTE:</strong> Because the log-likelihood <code>focei</code> calculation is different than using the NONMEM approximation of the individual Hessian (as described by Almquist 2015 <span class="citation">(<a href="#ref-Almquist2015" role="doc-biblioref">2</a>)</span>), <strong>you should not do statistical comparisons between a log-likelihood model and a standard focei model; The calculations are different, and it is not clear that the differences are due to likelihood alone.</strong></p>
<p>That being said, the generalized log-likelihood method <strong><em>does</em></strong> approximate the same likelihood as the NOMMEM-style <code>focei</code>. Hence, plotting the objective functions between the two methods give very similar values for each problem:</p>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/objf-1.png" width="672" /></p>
<div id="overall-conclusions-about-nlmixr2-log-likelihood-focei" class="section level3">
<h3>Overall Conclusions about nlmixr2 log-likelihood focei</h3>
<ul>
<li><p>The log-likelihood estimation procedure performs well for estimating population effects and between subject varaibilites</p></li>
<li><p>The log-likelihood estimation procedure does provide standard error estimates that are in the right general area, but tend to be a bit larger than the true values.</p></li>
<li><p>The procedure takes more time (0.8 to 7 fold more)</p></li>
<li><p>You should not compare objective functions between models that were estimated by general focei and log-likelihood focei.</p></li>
</ul>
</div>
</div>
<div id="generalized-likelihood-methods-in-nlmixr2" class="section level1">
<h1>Generalized likelihood methods in nlmixr2</h1>
<div id="normal-and-t-related-distributions" class="section level3">
<h3>Normal and t-related distributions</h3>
<p>For the normal and t-related distributions I wanted to keep the ability to use skew distributions, additive and proportional in the t-space etc, so these distributions are specified differently than the other supported distributions within <code>nlmixr2</code>:</p>
<table style="width:99%;">
<colgroup>
<col width="35%" />
<col width="23%" />
<col width="39%" />
</colgroup>
<thead>
<tr class="header">
<th>Distribution</th>
<th>How to Add</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Normal (log-likelihood)</td>
<td><code>+dnorm()</code></td>
<td><code>cp~add(add.sd)+dnorm()</code></td>
</tr>
<tr class="even">
<td>T-distribuion</td>
<td><code>+dt(df)</code></td>
<td><code>cp~add(add.sd)+dt(df)</code></td>
</tr>
<tr class="odd">
<td>Cauchy<br />
(t with df=1)</td>
<td><code>+dcauchy()</code></td>
<td><code>cp~add(add.sd)+dcauchy()</code></td>
</tr>
</tbody>
</table>
<p>Note that with the normal and t-related distributions <code>nlmixr2</code> will calculate the <code>cwres</code> and <code>npde</code> under the normal assumption to help assess the goodness of the fit of the model.</p>
</div>
<div id="more-distributions" class="section level3">
<h3>More distributions</h3>
<p>These (mostly) match all match the R distributions in the function. The left handed side of the equation (<code>err</code>) represent the compartment name for the <code>DV</code> in the dataset (if needed). It still needs the compartment defined even if it is a single endpoint model.</p>
<p>The only function that does not exactly the R documentation is the <code>dnbinomMu</code> probability function. It takes the <code>size</code> and <code>mu</code> as documented below and in the R documentation.</p>
<table style="width:89%;">
<colgroup>
<col width="47%" />
<col width="41%" />
</colgroup>
<thead>
<tr class="header">
<th>Distribution</th>
<th>How to Add</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Poision</td>
<td><code>err ~ dpois(lamba)</code></td>
</tr>
<tr class="even">
<td>Binomial</td>
<td><code>err ~ dbinom(n, p)</code></td>
</tr>
<tr class="odd">
<td>Beta</td>
<td><code>err ~ dbeta(alpha, beta)</code></td>
</tr>
<tr class="even">
<td>Chisq</td>
<td><code>err ~ chisq(nu)</code></td>
</tr>
<tr class="odd">
<td>Exponential</td>
<td><code>err ~ dexp(r)</code></td>
</tr>
<tr class="even">
<td>Uniform</td>
<td><code>err ~ dunif(a, b)</code></td>
</tr>
<tr class="odd">
<td>Weibull</td>
<td><code>err ~ dweibull(a, b)</code></td>
</tr>
<tr class="even">
<td>Gamma</td>
<td><code>err ~ dgamma(a, b)</code></td>
</tr>
<tr class="odd">
<td>Geom</td>
<td><code>err ~ dgeom(a)</code></td>
</tr>
<tr class="even">
<td>Negative Binomial</td>
<td><code>err ~ dnbinom(n, p)</code></td>
</tr>
<tr class="odd">
<td>Negative Binomial (mu version)</td>
<td><code>err ~ dnbinomMu(size, mu)</code></td>
</tr>
</tbody>
</table>
</div>
<div id="ordinal" class="section level3">
<h3>Ordinal</h3>
<p>Finally, ordinal likelihoods/simulations can be specified in 2 ways, the first is:</p>
<pre class="r"><code>err ~ c(p0, p1, p2)</code></pre>
<p>Here err represents the compartment and <code>p0</code> is the probability of being in category:</p>
<table>
<thead>
<tr class="header">
<th>Category</th>
<th>Probability</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>p0</td>
</tr>
<tr class="even">
<td>2</td>
<td>p1</td>
</tr>
<tr class="odd">
<td>3</td>
<td>p2</td>
</tr>
<tr class="even">
<td>4</td>
<td>1-p0-p1-p2</td>
</tr>
</tbody>
</table>
<p>It is up to the model to ensure that the sum of the <code>p</code> values are less than <code>1</code>. Additionally you can write an arbitrary number of categories in the ordinal model described above.</p>
<p>It seems a little off that <code>p0</code> is the probability for category <code>1</code> and sometimes scores are in non-whole numbers. This can be modeled as follows:</p>
<pre class="r"><code>err ~ c(p0=0, p1=1, p2=2, 3)</code></pre>
<p>Here the numeric categories are specified explicitly, and the probabilities remain the same:</p>
<table>
<thead>
<tr class="header">
<th>Category</th>
<th>Probability</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>p0</td>
</tr>
<tr class="even">
<td>1</td>
<td>p1</td>
</tr>
<tr class="odd">
<td>2</td>
<td>p2</td>
</tr>
<tr class="even">
<td>3</td>
<td>1-p0-p1-p2</td>
</tr>
</tbody>
</table>
</div>
<div id="generalized-likelihood" class="section level3">
<h3>Generalized likelihood</h3>
<p>The generalized log-likelihood can be defined as follows:</p>
<pre class="r"><code>ll(err) ~ log likelihood expression</code></pre>
<p>Here <code>err</code> represents the compartment or dvid that specifies the log-likelihood. You still need to specify it even if it is a single endpoint model (like the distributions above).</p>
</div>
<div id="references" class="section level3 unnumbered">
<h3>References</h3>
<div id="refs" class="references csl-bib-body">
<div id="ref-Schoemaker2019" class="csl-entry">
<div class="csl-left-margin">1. </div><div class="csl-right-inline">Schoemaker R, Fidler M, Laveille C, Wilkins JJ, Hooijmaijers R, Post TM, et al. <a href="https://doi.org/10.1002/psp4.12471"><span class="nocase">Performance of the SAEM and FOCEI Algorithms in the Open-Source, Nonlinear Mixed Effect Modeling Tool nlmixr</span></a>. CPT: Pharmacometrics and Systems Pharmacology. 2019;8(12):923â€“30. </div>
</div>
<div id="ref-Almquist2015" class="csl-entry">
<div class="csl-left-margin">2. </div><div class="csl-right-inline">Almquist J, Leander J, Jirstrand M. <a href="https://doi.org/10.1007/s10928-015-9409-1"><span class="nocase">Using sensitivity equations for computing gradients of the FOCE and FOCEI approximations to the population likelihood</span></a>. Journal of Pharmacokinetics and Pharmacodynamics. 2015 Jun;42(3):191â€“209. </div>
</div>
</div>
</div>
</div>
